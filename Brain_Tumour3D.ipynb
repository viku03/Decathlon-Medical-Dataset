{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from scipy.ndimage import rotate\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_slices(image, label, prediction=None, slice_idx=None):\n",
    "    \"\"\"Plot sample slices from the 3D volume for visual inspection.\"\"\"\n",
    "    if slice_idx is None:\n",
    "        slice_idx = image.shape[2] // 2\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3 if prediction is not None else 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot original image (first modality)\n",
    "    axes[0].imshow(image[:, :, slice_idx, 0], cmap='gray')\n",
    "    axes[0].set_title('Original Image (First Modality)')\n",
    "    \n",
    "    # Plot ground truth\n",
    "    axes[1].imshow(np.argmax(label[:, :, slice_idx], axis=-1), cmap='nipy_spectral')\n",
    "    axes[1].set_title('Ground Truth')\n",
    "    \n",
    "    if prediction is not None:\n",
    "        axes[2].imshow(np.argmax(prediction[:, :, slice_idx], axis=-1), cmap='nipy_spectral')\n",
    "        axes[2].set_title('Prediction')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_class_distribution(labels):\n",
    "    \"\"\"Analyze class distribution in the dataset.\"\"\"\n",
    "    class_counts = []\n",
    "    for label in labels:\n",
    "        unique, counts = np.unique(label, return_counts=True)\n",
    "        class_counts.append(dict(zip(unique, counts)))\n",
    "    \n",
    "    # Calculate average distribution\n",
    "    total_dist = {}\n",
    "    for d in class_counts:\n",
    "        for k, v in d.items():\n",
    "            total_dist[k] = total_dist.get(k, 0) + v\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(total_dist.keys(), [v/len(class_counts) for v in total_dist.values()])\n",
    "    plt.title('Average Class Distribution per Volume')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Average Number of Voxels')\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    return total_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainTumor3DDataset:\n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = base_path\n",
    "        self.dataset_json = self._load_dataset_json()\n",
    "        self.train_files = self._get_training_files()\n",
    "        \n",
    "    def _load_dataset_json(self):\n",
    "        json_path = os.path.join(self.base_path, 'ML_Decathlon_Dataset/Task01_BrainTumour/dataset.json')\n",
    "        with open(json_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    def _get_training_files(self):\n",
    "        return self.dataset_json['training']\n",
    "    \n",
    "    def validate_data_integrity(self, images, labels):\n",
    "        \"\"\"Validate data integrity and check for potential issues.\"\"\"\n",
    "        print(\"\\nData Validation Report:\")\n",
    "        print(\"-----------------------\")\n",
    "        \n",
    "        # Check shapes\n",
    "        print(f\"Number of samples: {len(images)}\")\n",
    "        print(f\"Image shape: {images[0].shape}\")\n",
    "        print(f\"Label shape: {labels[0].shape}\")\n",
    "        \n",
    "        # Check value ranges\n",
    "        for i, (img, lbl) in enumerate(zip(images, labels)):\n",
    "            print(f\"\\nSample {i}:\")\n",
    "            print(f\"Image value range: [{np.min(img):.3f}, {np.max(img):.3f}]\")\n",
    "            print(f\"Unique labels: {np.unique(lbl)}\")\n",
    "            \n",
    "            # Check for NaN/Inf\n",
    "            if np.any(np.isnan(img)) or np.any(np.isinf(img)):\n",
    "                print(\"WARNING: Found NaN or Inf values in image!\")\n",
    "            \n",
    "            # Check label validity\n",
    "            if not np.array_equal(np.unique(lbl), np.arange(len(np.unique(lbl)))):\n",
    "                print(\"WARNING: Labels might not be consecutive integers!\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def load_volume(self, file_path):\n",
    "        full_path = os.path.join(self.base_path, 'ML_Decathlon_Dataset/Task01_BrainTumour', \n",
    "                                file_path.replace('./', ''))\n",
    "        return nib.load(full_path).get_fdata()\n",
    "\n",
    "    def analyze_class_distribution(self, labels):\n",
    "        \"\"\"Analyze class distribution in the dataset\"\"\"\n",
    "        class_counts = {}\n",
    "        total_voxels = 0\n",
    "        \n",
    "        for label_volume in labels:\n",
    "            unique, counts = np.unique(label_volume, return_counts=True)\n",
    "            total_voxels += label_volume.size\n",
    "            \n",
    "            for class_idx, count in zip(unique, counts):\n",
    "                if class_idx not in class_counts:\n",
    "                    class_counts[class_idx] = 0\n",
    "                class_counts[class_idx] += count\n",
    "        \n",
    "        # Convert to percentages\n",
    "        class_percentages = {k: (v/total_voxels)*100 for k, v in class_counts.items()}\n",
    "        \n",
    "        return class_counts, class_percentages\n",
    "\n",
    "    def preprocess_volume(self, volume):\n",
    "        \"\"\"Optimized preprocessing using vectorized operations\"\"\"\n",
    "        preprocessed = np.zeros_like(volume, dtype=np.float32)\n",
    "        for i in range(volume.shape[-1]):\n",
    "            modality = volume[..., i]\n",
    "            nonzero_mask = modality != 0\n",
    "            if np.any(nonzero_mask):\n",
    "                mean = np.mean(modality[nonzero_mask])\n",
    "                std = np.std(modality[nonzero_mask])\n",
    "                if std != 0:\n",
    "                    preprocessed[..., i] = (modality - mean) / std\n",
    "        return preprocessed\n",
    "\n",
    "    def prepare_data(self, num_samples=None):\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        train_files = self.train_files[:num_samples] if num_samples else self.train_files\n",
    "        \n",
    "        for idx, file_info in tqdm(enumerate(train_files), desc=\"Loading data\", total=len(train_files)):\n",
    "            try:\n",
    "                image = self.load_volume(file_info['image'])\n",
    "                label = self.load_volume(file_info['label'])\n",
    "                \n",
    "                # Check for potential data issues\n",
    "                if np.any(np.isnan(image)) or np.any(np.isinf(image)):\n",
    "                    print(f\"Warning: Found NaN or Inf values in image {idx}\")\n",
    "                    continue\n",
    "                \n",
    "                image = self.preprocess_volume(image)\n",
    "                \n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "                \n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {idx}: {e}\")\n",
    "                continue\n",
    "            \n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator3D(Sequence):\n",
    "    def __init__(self, image_list, label_list, batch_size=1, patch_size=(64, 64, 64),\n",
    "                 n_channels=4, n_classes=4, shuffle=True, augment=False):\n",
    "        self.image_list = image_list\n",
    "        self.label_list = label_list\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Initialize valid_patches before calling on_epoch_end\n",
    "        self.valid_patches = []\n",
    "        self._calculate_valid_patches()\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        # Validate patch extraction\n",
    "        self._validate_patch_extraction()\n",
    "    \n",
    "    def _validate_patch_extraction(self):\n",
    "        \"\"\"Validate the first few patches to ensure correct extraction.\"\"\"\n",
    "        print(\"\\nPatch Validation:\")\n",
    "        print(\"----------------\")\n",
    "        \n",
    "        for i in range(min(3, len(self.valid_patches))):\n",
    "            img_idx, x, y, z = self.valid_patches[i]\n",
    "            patch_x, patch_y = self._extract_patch(img_idx, x, y, z)\n",
    "            \n",
    "            print(f\"\\nPatch {i}:\")\n",
    "            print(f\"Shape: {patch_x.shape}\")\n",
    "            print(f\"Value range: [{np.min(patch_x):.3f}, {np.max(patch_x):.3f}]\")\n",
    "            print(f\"Unique labels: {np.unique(np.argmax(patch_y, axis=-1))}\")\n",
    "\n",
    "    def analyze_patch_distribution(self):\n",
    "        \"\"\"Analyze class distribution in selected patches\"\"\"\n",
    "        class_counts = np.zeros(self.n_classes)\n",
    "        for idx, (img_idx, x, y, z) in enumerate(self.valid_patches[:100]):  # Sample first 100 patches\n",
    "            label_patch = self.label_list[img_idx][\n",
    "                x:x + self.patch_size[0],\n",
    "                y:y + self.patch_size[1],\n",
    "                z:z + self.patch_size[2]\n",
    "            ]\n",
    "            unique, counts = np.unique(label_patch, return_counts=True)\n",
    "            for class_idx, count in zip(unique, counts):\n",
    "                class_counts[int(class_idx)] += count\n",
    "                \n",
    "        total_voxels = np.sum(class_counts)\n",
    "        class_percentages = (class_counts / total_voxels) * 100\n",
    "        \n",
    "        print(\"\\nPatch-wise class distribution:\")\n",
    "        for i, percentage in enumerate(class_percentages):\n",
    "            print(f\"Class {i}: {percentage:.2f}%\")\n",
    "\n",
    "    def _calculate_valid_patches(self):\n",
    "        valid_patches = []\n",
    "        stride = [p // 2 for p in self.patch_size]\n",
    "        \n",
    "        for idx, image in enumerate(self.image_list):\n",
    "            x_coords = range(0, image.shape[0] - self.patch_size[0], stride[0])\n",
    "            y_coords = range(0, image.shape[1] - self.patch_size[1], stride[1])\n",
    "            z_coords = range(0, image.shape[2] - self.patch_size[2], stride[2])\n",
    "            \n",
    "            for x in x_coords:\n",
    "                for y in y_coords:\n",
    "                    for z in z_coords:\n",
    "                        patch = image[x:x + self.patch_size[0],\n",
    "                                    y:y + self.patch_size[1],\n",
    "                                    z:z + self.patch_size[2]]\n",
    "                        if np.any(patch):\n",
    "                            valid_patches.append((idx, x, y, z))\n",
    "        \n",
    "        self.valid_patches = valid_patches\n",
    "        print(f\"Total valid patches: {len(self.valid_patches)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.valid_patches) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = min((index + 1) * self.batch_size, len(self.valid_patches))\n",
    "        batch_patches = self.valid_patches[start_idx:end_idx]\n",
    "        \n",
    "        batch_size = len(batch_patches)\n",
    "        X = np.zeros((batch_size, *self.patch_size, self.n_channels), dtype=np.float32)\n",
    "        y = np.zeros((batch_size, *self.patch_size, self.n_classes), dtype=np.float32)\n",
    "\n",
    "        for i, (img_idx, x, y_coord, z) in enumerate(batch_patches):\n",
    "            X[i], y[i] = self._extract_patch(img_idx, x, y_coord, z)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def _extract_patch(self, img_idx, x, y, z):\n",
    "        image = self.image_list[img_idx]\n",
    "        label = self.label_list[img_idx]\n",
    "\n",
    "        patch_x = image[x:x + self.patch_size[0],\n",
    "                       y:y + self.patch_size[1],\n",
    "                       z:z + self.patch_size[2]].astype(np.float32)\n",
    "\n",
    "        patch_y = np.zeros((*self.patch_size, self.n_classes), dtype=np.float32)\n",
    "        for c in range(self.n_classes):\n",
    "            patch_y[..., c] = (label[x:x + self.patch_size[0],\n",
    "                                    y:y + self.patch_size[1],\n",
    "                                    z:z + self.patch_size[2]] == c)\n",
    "\n",
    "        if self.augment:\n",
    "            patch_x, patch_y = self._augment_data(patch_x, patch_y)\n",
    "\n",
    "        return patch_x, patch_y\n",
    "\n",
    "    @staticmethod\n",
    "    def _augment_data(image, label):\n",
    "        if np.random.random() > 0.5:\n",
    "            angle = np.random.uniform(-20, 20)\n",
    "            image = np.stack([rotate(image[..., c], angle, axes=(0, 1), reshape=False)\n",
    "                            for c in range(image.shape[-1])], axis=-1)\n",
    "            label = np.stack([rotate(label[..., c], angle, axes=(0, 1), reshape=False)\n",
    "                            for c in range(label.shape[-1])], axis=-1)\n",
    "        return image, label\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.valid_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import signal\n",
    "# import sys\n",
    "\n",
    "# class ModelSaver:\n",
    "#     def __init__(self):\n",
    "#         self.model = None\n",
    "#         self.stop_training = False\n",
    "#         self.history = None\n",
    "        \n",
    "#     def save_model(self, signum, frame):\n",
    "#         print('\\nCtrl+C detected. Saving model and training history...')\n",
    "#         self.stop_training = True\n",
    "#         if self.model is not None:\n",
    "#             # Save model\n",
    "#             self.model.save('interrupted_model.keras')\n",
    "#             print('Model saved as interrupted_model.keras')\n",
    "            \n",
    "#             # Save training history if it exists\n",
    "#             if self.history is not None:\n",
    "#                 import pickle\n",
    "#                 with open('training_history.pkl', 'wb') as f:\n",
    "#                     pickle.dump(self.history.history, f)\n",
    "#                 print('Training history saved as training_history.pkl')\n",
    "#         sys.exit(0)\n",
    "\n",
    "# def train_model(validation_split=0.2, max_epochs=30, num_samples=10):\n",
    "#     \"\"\"\n",
    "#     Train the 3D U-Net model with the ability to save on interrupt\n",
    "#     \"\"\"\n",
    "#     # Initialize model saver and set up interrupt handler\n",
    "#     model_saver = ModelSaver()\n",
    "#     signal.signal(signal.SIGINT, model_saver.save_model)\n",
    "    \n",
    "#     try:\n",
    "#         # Initialize dataset\n",
    "#         dataset = BrainTumor3DDataset(base_path='.')\n",
    "        \n",
    "#         print(\"Starting data preparation...\")\n",
    "#         images, labels = dataset.prepare_data(num_samples=num_samples)\n",
    "        \n",
    "#         # Analyze class distribution\n",
    "#         print(\"\\nAnalyzing class distribution...\")\n",
    "#         class_dist = analyze_class_distribution(labels)\n",
    "        \n",
    "#         # Validate data integrity\n",
    "#         dataset.validate_data_integrity(images, labels)\n",
    "        \n",
    "#         print(\"\\nCreating train/val split...\")\n",
    "#         X_train, X_val, y_train, y_val = train_test_split(\n",
    "#             images, labels, test_size=validation_split, random_state=42\n",
    "#         )\n",
    "        \n",
    "#         # Clear memory\n",
    "#         del images, labels\n",
    "#         gc.collect()\n",
    "        \n",
    "#         # Calculate class weights based on distribution\n",
    "#         total_pixels = sum(class_dist.values())\n",
    "#         class_weights = {\n",
    "#             i: total_pixels / (len(class_dist) * count)\n",
    "#             for i, count in class_dist.items()\n",
    "#         }\n",
    "        \n",
    "#         # Create data generators\n",
    "#         patch_size = (64, 64, 64)\n",
    "#         train_generator = DataGenerator3D(\n",
    "#             X_train, y_train, batch_size=1, patch_size=patch_size, augment=True\n",
    "#         )\n",
    "#         val_generator = DataGenerator3D(\n",
    "#             X_val, y_val, batch_size=1, patch_size=patch_size, augment=False\n",
    "#         )\n",
    "        \n",
    "#         # Create and compile model\n",
    "#         input_shape = (*patch_size, 4)\n",
    "#         model = create_3d_unet(input_shape, n_classes=4)\n",
    "        \n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, epsilon=1e-8)\n",
    "        \n",
    "#         model.compile(\n",
    "#             optimizer=optimizer,\n",
    "#             loss=dice_loss,\n",
    "#             metrics=['accuracy', tf.keras.metrics.MeanIoU(num_classes=4)]\n",
    "#         )\n",
    "        \n",
    "#         # Store model in model_saver\n",
    "#         model_saver.model = model\n",
    "        \n",
    "#         # Create a custom callback for periodic saving\n",
    "#         class PeriodicModelSaver(tf.keras.callbacks.Callback):\n",
    "#             def __init__(self, save_freq=100):  # save_freq in steps\n",
    "#                 super().__init__()\n",
    "#                 self.save_freq = save_freq\n",
    "#                 self.steps = 0\n",
    "                \n",
    "#             def on_batch_end(self, batch, logs=None):\n",
    "#                 self.steps += 1\n",
    "#                 if self.steps % self.save_freq == 0:\n",
    "#                     self.model.save(f'model_checkpoint_step_{self.steps}.keras')\n",
    "#                     print(f'\\nSaved model at step {self.steps}')\n",
    "        \n",
    "#         # Enhanced callbacks\n",
    "#         callbacks = [\n",
    "#             tf.keras.callbacks.EarlyStopping(\n",
    "#                 monitor='val_loss',\n",
    "#                 patience=5,\n",
    "#                 min_delta=0.001,\n",
    "#                 restore_best_weights=True,\n",
    "#                 verbose=1\n",
    "#             ),\n",
    "#             tf.keras.callbacks.ModelCheckpoint(\n",
    "#                 'best_3d_model.keras',\n",
    "#                 save_best_only=True,\n",
    "#                 monitor='val_loss',\n",
    "#                 mode='min',\n",
    "#                 verbose=1\n",
    "#             ),\n",
    "#             tf.keras.callbacks.ReduceLROnPlateau(\n",
    "#                 monitor='val_loss',\n",
    "#                 factor=0.5,\n",
    "#                 patience=3,\n",
    "#                 min_lr=1e-6,\n",
    "#                 mode='min',\n",
    "#                 verbose=1\n",
    "#             ),\n",
    "#             tf.keras.callbacks.CSVLogger('training_log.csv'),\n",
    "#             PeriodicModelSaver(save_freq=100),  # Save every 100 steps\n",
    "#             tf.keras.callbacks.LambdaCallback(\n",
    "#                 on_epoch_end=lambda epoch, logs: gc.collect()\n",
    "#             )\n",
    "#         ]\n",
    "        \n",
    "#         # Train model\n",
    "#         history = model.fit(\n",
    "#             train_generator,\n",
    "#             validation_data=val_generator,\n",
    "#             epochs=max_epochs,\n",
    "#             callbacks=callbacks,\n",
    "#             class_weight=class_weights\n",
    "#         )\n",
    "        \n",
    "#         # Store history in model_saver\n",
    "#         model_saver.history = history\n",
    "        \n",
    "#         return model, history\n",
    "    \n",
    "#     except KeyboardInterrupt:\n",
    "#         print('\\nTraining interrupted by user. Saving model...')\n",
    "#         if model_saver.model is not None:\n",
    "#             model_saver.model.save('interrupted_model.keras')\n",
    "#             print('Model saved as interrupted_model.keras')\n",
    "            \n",
    "#             # Save partial history if it exists\n",
    "#             if hasattr(model_saver.model, 'history') and model_saver.model.history is not None:\n",
    "#                 import pickle\n",
    "#                 with open('partial_training_history.pkl', 'wb') as f:\n",
    "#                     pickle.dump(model_saver.model.history.history, f)\n",
    "#                 print('Partial training history saved as partial_training_history.pkl')\n",
    "        \n",
    "#         return model_saver.model, model_saver.model.history if hasattr(model_saver.model, 'history') else None\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f'\\nAn error occurred during training: {str(e)}')\n",
    "#         if model_saver.model is not None:\n",
    "#             model_saver.model.save('error_model.keras')\n",
    "#             print('Model saved as error_model.keras')\n",
    "#         raise e\n",
    "\n",
    "# # To load a saved model later:\n",
    "# def load_saved_model(model_path='interrupted_model.keras', history_path='training_history.pkl'):\n",
    "#     \"\"\"\n",
    "#     Load a saved model and its training history\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Load model\n",
    "#         model = tf.keras.models.load_model(model_path, custom_objects={'dice_loss': dice_loss})\n",
    "#         print(f'Model loaded from {model_path}')\n",
    "        \n",
    "#         # Try to load history\n",
    "#         try:\n",
    "#             import pickle\n",
    "#             with open(history_path, 'rb') as f:\n",
    "#                 history = pickle.load(f)\n",
    "#             print(f'Training history loaded from {history_path}')\n",
    "#             return model, history\n",
    "#         except:\n",
    "#             print('No training history found')\n",
    "#             return model, None\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f'Error loading model: {str(e)}')\n",
    "#         return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "def create_3d_unet(input_shape, n_classes=4, n_filters=16):\n",
    "    \"\"\"Create 3D U-Net model architecture\"\"\"\n",
    "    inputs = tf.keras.Input(input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = conv_block_3d(inputs, n_filters)\n",
    "    pool1 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv1)\n",
    "    \n",
    "    conv2 = conv_block_3d(pool1, n_filters*2)\n",
    "    pool2 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n",
    "    \n",
    "    conv3 = conv_block_3d(pool2, n_filters*4)\n",
    "    pool3 = tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2))(conv3)\n",
    "    \n",
    "    # Bridge\n",
    "    conv4 = conv_block_3d(pool3, n_filters*8)\n",
    "    \n",
    "    # Decoder\n",
    "    up5 = tf.keras.layers.Conv3DTranspose(n_filters*4, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv4)\n",
    "    concat5 = tf.keras.layers.concatenate([up5, conv3])\n",
    "    conv5 = conv_block_3d(concat5, n_filters*4)\n",
    "    \n",
    "    up6 = tf.keras.layers.Conv3DTranspose(n_filters*2, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv5)\n",
    "    concat6 = tf.keras.layers.concatenate([up6, conv2])\n",
    "    conv6 = conv_block_3d(concat6, n_filters*2)\n",
    "    \n",
    "    up7 = tf.keras.layers.Conv3DTranspose(n_filters, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv6)\n",
    "    concat7 = tf.keras.layers.concatenate([up7, conv1])\n",
    "    conv7 = conv_block_3d(concat7, n_filters)\n",
    "    \n",
    "    # Output\n",
    "    outputs = tf.keras.layers.Conv3D(n_classes, (1, 1, 1), activation='softmax')(conv7)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "def conv_block_3d(inputs, n_filters, kernel_size=(3, 3, 3)):\n",
    "    \"\"\"Create a 3D convolution block\"\"\"\n",
    "    x = tf.keras.layers.Conv3D(n_filters, kernel_size, padding='same')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv3D(n_filters, kernel_size, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    \"\"\"Calculate Dice loss\"\"\"\n",
    "    smooth = 1e-5\n",
    "    \n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred) + smooth\n",
    "    denominator = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth\n",
    "    \n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "class TrainingController:\n",
    "    def __init__(self):\n",
    "        self.stop_training = False\n",
    "        self.button = widgets.Button(description='Stop Training')\n",
    "        self.button.on_click(self.on_button_clicked)\n",
    "        display(self.button)\n",
    "    \n",
    "    def on_button_clicked(self, b):\n",
    "        self.stop_training = True\n",
    "        print(\"\\nStopping training after current epoch completes...\")\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, controller):\n",
    "        super().__init__()\n",
    "        self.controller = controller\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.controller.stop_training:\n",
    "            self.model.stop_training = True\n",
    "            print(\"\\nSaving model...\")\n",
    "            self.model.save(f'model_stopped_epoch_{epoch}.keras')\n",
    "\n",
    "def train_model(validation_split=0.2, max_epochs=30, num_samples=10):\n",
    "    \"\"\"Train the 3D U-Net model with stop button functionality\"\"\"\n",
    "    try:\n",
    "        # Initialize controller for stop button\n",
    "        controller = TrainingController()\n",
    "        \n",
    "        # Initialize dataset\n",
    "        dataset = BrainTumor3DDataset(base_path='.')\n",
    "        \n",
    "        print(\"Starting data preparation...\")\n",
    "        images, labels = dataset.prepare_data(num_samples=num_samples)\n",
    "        \n",
    "        # Analyze class distribution\n",
    "        print(\"\\nAnalyzing class distribution...\")\n",
    "        class_dist = analyze_class_distribution(labels)\n",
    "        \n",
    "        # Validate data integrity\n",
    "        dataset.validate_data_integrity(images, labels)\n",
    "        \n",
    "        print(\"\\nCreating train/val split...\")\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            images, labels, test_size=validation_split, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Clear memory\n",
    "        del images, labels\n",
    "        gc.collect()\n",
    "        \n",
    "        # Calculate class weights\n",
    "        total_pixels = sum(class_dist.values())\n",
    "        class_weights = {\n",
    "            i: total_pixels / (len(class_dist) * count)\n",
    "            for i, count in class_dist.items()\n",
    "        }\n",
    "        \n",
    "        # Create data generators\n",
    "        patch_size = (64, 64, 64)\n",
    "        train_generator = DataGenerator3D(\n",
    "            X_train, y_train, batch_size=1, patch_size=patch_size, augment=True, class_weights=class_weights\n",
    "        )\n",
    "        val_generator = DataGenerator3D(\n",
    "            X_val, y_val, batch_size=1, patch_size=patch_size, augment=False\n",
    "        )\n",
    "        \n",
    "        # Create and compile model\n",
    "        input_shape = (*patch_size, 4)\n",
    "        model = create_3d_unet(input_shape, n_classes=4)\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, epsilon=1e-8)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=dice_loss,\n",
    "            metrics=['accuracy', tf.keras.metrics.MeanIoU(num_classes=4)]\n",
    "        )\n",
    "        \n",
    "        # Enhanced callbacks with stop button\n",
    "        callbacks = [\n",
    "            CustomCallback(controller),\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=5,\n",
    "                min_delta=0.001,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                'best_3d_model.keras',\n",
    "                save_best_only=True,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=3,\n",
    "                min_lr=1e-6,\n",
    "                mode='min',\n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.CSVLogger('training_log.csv'),\n",
    "            tf.keras.callbacks.LambdaCallback(\n",
    "                on_epoch_end=lambda epoch, logs: gc.collect()\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            validation_data=val_generator,\n",
    "            epochs=max_epochs,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        return model, history\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'\\nAn error occurred during training: {str(e)}')\n",
    "        if 'model' in locals():\n",
    "            model.save('error_model.keras')\n",
    "            print('Model saved as error_model.keras')\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_generator):\n",
    "    \"\"\"Evaluate model performance with detailed metrics.\"\"\"\n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    print(\"----------------\")\n",
    "    \n",
    "    # Get predictions for validation set\n",
    "    val_predictions = []\n",
    "    val_true = []\n",
    "    \n",
    "    for i in range(len(val_generator)):\n",
    "        x, y = val_generator[i]\n",
    "        pred = model.predict(x)\n",
    "        val_predictions.append(pred)\n",
    "        val_true.append(y)\n",
    "    \n",
    "    val_pred = np.concatenate(val_predictions)\n",
    "    val_true = np.concatenate(val_true)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(np.argmax(val_pred, axis=-1) == np.argmax(val_true, axis=-1))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        np.argmax(val_true.reshape(-1, val_true.shape[-1]), axis=-1),\n",
    "        np.argmax(val_pred.reshape(-1, val_pred.shape[-1]), axis=-1)\n",
    "    )\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    for i in range(val_true.shape[-1]):\n",
    "        class_acc = np.mean(\n",
    "            np.argmax(val_pred, axis=-1)[np.argmax(val_true, axis=-1) == i] == i\n",
    "        )\n",
    "        print(f\"Class {i} Accuracy: {class_acc:.4f}\")\n",
    "    \n",
    "    return accuracy, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78115252ae504c0da3898bb51737fbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Stop Training', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preparation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|██████████| 10/10 [00:04<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing class distribution...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Validation Report:\n",
      "-----------------------\n",
      "Number of samples: 10\n",
      "Image shape: (240, 240, 155, 4)\n",
      "Label shape: (240, 240, 155)\n",
      "\n",
      "Sample 0:\n",
      "Image value range: [-5.103, 10.282]\n",
      "Unique labels: [0. 1. 2. 3.]\n",
      "\n",
      "Sample 1:\n",
      "Image value range: [-3.452, 12.930]\n",
      "Unique labels: [0. 1. 2. 3.]\n",
      "\n",
      "Sample 2:\n",
      "Image value range: [-4.458, 11.208]\n",
      "Unique labels: [0. 1. 2. 3.]\n",
      "\n",
      "Sample 3:\n",
      "Image value range: [-4.506, 12.677]\n",
      "Unique labels: [0. 1. 2. 3.]\n",
      "\n",
      "Sample 4:\n",
      "Image value range: [-3.708, 12.848]\n",
      "Unique labels: [0. 1. 2. 3.]\n",
      "\n",
      "Sample 5:\n",
      "Image value range: [-5.864, 10.379]\n",
      "Unique labels: [0. 1. 2. 3.]\n",
      "\n",
      "Sample 6:\n",
      "Image value range: [-4.986, 13.716]\n",
      "Unique labels: [0. 1. 2. 3.]\n",
      "\n",
      "Sample 7:\n",
      "Image value range: [-3.634, 12.997]\n",
      "Unique labels: [0. 1. 2. 3.]\n",
      "\n",
      "Sample 8:\n",
      "Image value range: [-5.502, 11.336]\n",
      "Unique labels: [0. 1. 2. 3.]\n",
      "\n",
      "Sample 9:\n",
      "Image value range: [-4.710, 11.241]\n",
      "Unique labels: [0. 1. 2. 3.]\n",
      "\n",
      "Creating train/val split...\n",
      "\n",
      "An error occurred during training: DataGenerator3D.__init__() got an unexpected keyword argument 'class_weights'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DataGenerator3D.__init__() got an unexpected keyword argument 'class_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m         tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mset_memory_growth(device, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[0;32mIn[23], line 191\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(validation_split, max_epochs, num_samples)\u001b[0m\n\u001b[1;32m    189\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel saved as error_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[23], line 126\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(validation_split, max_epochs, num_samples)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Create data generators\u001b[39;00m\n\u001b[1;32m    125\u001b[0m patch_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m \u001b[43mDataGenerator3D\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m val_generator \u001b[38;5;241m=\u001b[39m DataGenerator3D(\n\u001b[1;32m    130\u001b[0m     X_val, y_val, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, patch_size\u001b[38;5;241m=\u001b[39mpatch_size, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Create and compile model\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: DataGenerator3D.__init__() got an unexpected keyword argument 'class_weights'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set memory growth for GPU\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if physical_devices:\n",
    "        for device in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "    \n",
    "    # Train model\n",
    "    model, history = train_model(num_samples=10)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history.history['mean_io_u'], label='Training IoU')\n",
    "    plt.plot(history.history['val_mean_io_u'], label='Validation IoU')\n",
    "    plt.title('Mean IoU')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history_3d.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
